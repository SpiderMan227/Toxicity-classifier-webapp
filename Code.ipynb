{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a2fefa",
   "metadata": {},
   "source": [
    "# Multi-Label Toxic Comment Classifier\n",
    "Complete step-by-step pipeline using PyTorch + HuggingFace DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3d2e787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All packages installed\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    \"torch\",\n",
    "    \"transformers\",\n",
    "    \"pandas\",\n",
    "    \"numpy\",\n",
    "    \"scikit-learn\",\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "print(\"✓ All packages installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1c312d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "524a37f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset loaded\n",
      "Shape: (159571, 8)\n",
      "Columns: ['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "\n",
      "First 3 rows:\n",
      "                 id                                       comment_text  toxic  \\\n",
      "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
      "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
      "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  \n",
      "0             0        0       0       0              0  \n",
      "1             0        0       0       0              0  \n",
      "2             0        0       0       0              0  \n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load the CSV file\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "print(f\"✓ Dataset loaded\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bade6d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label statistics:\n",
      "toxic            15294\n",
      "severe_toxic      1595\n",
      "obscene           8449\n",
      "threat             478\n",
      "insult            7877\n",
      "identity_hate     1405\n",
      "dtype: int64\n",
      "\n",
      "Missing values:\n",
      "id               0\n",
      "comment_text     0\n",
      "toxic            0\n",
      "severe_toxic     0\n",
      "obscene          0\n",
      "threat           0\n",
      "insult           0\n",
      "identity_hate    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Explore the data\n",
    "label_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "print(\"Label statistics:\")\n",
    "print(df[label_cols].sum())\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c5fe275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Text preprocessing complete\n",
      "\n",
      "Example text:\n",
      "Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Preprocess text - handle NaN and strip whitespace\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Handle NaN values and strip whitespace\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return str(text).strip()\n",
    "\n",
    "df[\"comment_text\"] = df[\"comment_text\"].apply(preprocess_text)\n",
    "\n",
    "print(\"✓ Text preprocessing complete\")\n",
    "print(f\"\\nExample text:\\n{df['comment_text'].iloc[0][:150]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d9a9e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenizer loaded: distilbert-base-uncased\n",
      "Vocab size: 30522\n",
      "Max length: 128\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Load tokenizer\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"✓ Tokenizer loaded: {MODEL_NAME}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Max length: {MAX_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77165331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenization test on sample text\n",
      "Input text: Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't ...\n",
      "Input IDs shape: torch.Size([1, 128])\n",
      "Attention mask shape: torch.Size([1, 128])\n",
      "Sample input IDs: [101, 7526, 2339, 1996, 10086, 2015, 2081, 2104, 2026, 5310]\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Test tokenization on one example\n",
    "sample_text = df[\"comment_text\"].iloc[0]\n",
    "\n",
    "sample_encoding = tokenizer(\n",
    "    sample_text,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=MAX_LENGTH,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "print(\"✓ Tokenization test on sample text\")\n",
    "print(f\"Input text: {sample_text[:100]}...\")\n",
    "print(f\"Input IDs shape: {sample_encoding['input_ids'].shape}\")\n",
    "print(f\"Attention mask shape: {sample_encoding['attention_mask'].shape}\")\n",
    "print(f\"Sample input IDs: {sample_encoding['input_ids'][0][:10].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4c88935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Custom Dataset class created\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Create custom PyTorch Dataset class\n",
    "class ToxicDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for multi-label toxic comments\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.float32),\n",
    "        }\n",
    "\n",
    "print(\"✓ Custom Dataset class created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24140e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels shape: (159571, 6)\n",
      "Texts count: 159571\n",
      "Sample labels: [0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "✓ Dataset created with 159571 samples\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Prepare labels and create dataset\n",
    "# Labels shape: (num_samples, 6) for 6 toxic label types\n",
    "labels = df[label_cols].values.astype(np.float32)\n",
    "texts = df[\"comment_text\"].tolist()\n",
    "\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print(f\"Texts count: {len(texts)}\")\n",
    "print(f\"Sample labels: {labels[0]}\")\n",
    "\n",
    "# Create dataset\n",
    "dataset = ToxicDataset(texts, labels, tokenizer, MAX_LENGTH)\n",
    "\n",
    "print(f\"\\n✓ Dataset created with {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ebec152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DataLoader created\n",
      "  Batch size: 16\n",
      "  Number of batches: 9974\n",
      "\n",
      "Sample batch shapes:\n",
      "  input_ids: torch.Size([16, 128])\n",
      "  attention_mask: torch.Size([16, 128])\n",
      "  labels: torch.Size([16, 6])\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Create DataLoader with default batch size\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "print(f\"✓ DataLoader created\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Number of batches: {len(train_loader)}\")\n",
    "\n",
    "# Test one batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nSample batch shapes:\")\n",
    "print(f\"  input_ids: {sample_batch['input_ids'].shape}\")\n",
    "print(f\"  attention_mask: {sample_batch['attention_mask'].shape}\")\n",
    "print(f\"  labels: {sample_batch['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3dc35457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded: distilbert-base-uncased\n",
      "Number of labels: 6\n",
      "Problem type: multi_label_classification\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Setup device and load model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"✓ Using device: {device}\")\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=6,\n",
    "    problem_type=\"multi_label_classification\",\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(f\"✓ Model loaded: {MODEL_NAME}\")\n",
    "print(f\"Number of labels: 6\")\n",
    "print(f\"Problem type: multi_label_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e640e4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ GPU Memory Check\n",
      "  Total GPU memory: 6.44 GB\n",
      "  Allocated memory: 1.37 GB\n",
      "  Reserved memory: 1.81 GB\n",
      "\n",
      "✓ Testing batch sizes for RTX 3050...\n",
      "  Batch size  16: ✓ OK\n",
      "  Batch size  24: ✓ OK\n",
      "  Batch size  32: ✓ OK\n",
      "  Batch size  48: ✓ OK\n",
      "  Batch size  64: ✓ OK\n",
      "  Batch size  96: ✓ OK\n",
      "  Batch size 128: ✓ OK\n",
      "\n",
      "✓ Updated DataLoader with optimal batch size: 128\n",
      "  Number of batches: 1247\n"
     ]
    }
   ],
   "source": [
    "# Step 11b: Check GPU memory and find optimal batch size\n",
    "print(f\"\\n✓ GPU Memory Check\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"  Allocated memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"  Reserved memory: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Test different batch sizes to find maximum\n",
    "    def test_batch_size(bs):\n",
    "        \"\"\"Test if batch_size fits in GPU memory\"\"\"\n",
    "        try:\n",
    "            test_loader = DataLoader(dataset, batch_size=bs, shuffle=False)\n",
    "            sample_batch = next(iter(test_loader))\n",
    "            \n",
    "            input_ids = sample_batch[\"input_ids\"].to(device)\n",
    "            attention_mask = sample_batch[\"attention_mask\"].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            return True\n",
    "        except RuntimeError:\n",
    "            torch.cuda.empty_cache()\n",
    "            return False\n",
    "    \n",
    "    print(f\"\\n✓ Testing batch sizes for RTX 3050...\")\n",
    "    max_batch_size = 16\n",
    "    \n",
    "    for bs in [16, 24, 32, 48, 64, 96, 128]:\n",
    "        if test_batch_size(bs):\n",
    "            max_batch_size = bs\n",
    "            print(f\"  Batch size {bs:3d}: ✓ OK\")\n",
    "        else:\n",
    "            print(f\"  Batch size {bs:3d}: ✗ OUT OF MEMORY\")\n",
    "            break\n",
    "    \n",
    "    # Recreate DataLoader with optimal batch size\n",
    "    BATCH_SIZE = max_batch_size\n",
    "    train_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Updated DataLoader with optimal batch size: {BATCH_SIZE}\")\n",
    "    print(f\"  Number of batches: {len(train_loader)}\")\n",
    "else:\n",
    "    print(\"  CPU mode - no GPU optimization needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53af336f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loss function: BCEWithLogitsLoss\n",
      "  (suitable for multi-label classification)\n"
     ]
    }
   ],
   "source": [
    "# Step 12: Setup loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(f\"✓ Loss function: BCEWithLogitsLoss\")\n",
    "print(\"  (suitable for multi-label classification)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92af9dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Optimizer: AdamW\n",
      "  Learning rate: 2e-05\n",
      "✓ Scheduler: Linear warmup\n",
      "  Total training steps: 3741\n"
     ]
    }
   ],
   "source": [
    "# Step 13: Setup optimizer and scheduler\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps,\n",
    ")\n",
    "\n",
    "print(f\"✓ Optimizer: AdamW\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"✓ Scheduler: Linear warmup\")\n",
    "print(f\"  Total training steps: {total_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d408401a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1/3 ---\n",
      "  Batch   20 | Loss: 0.0360 | Avg Loss: 0.0447\n",
      "  Batch   20 | Loss: 0.0360 | Avg Loss: 0.0447\n",
      "  Batch   40 | Loss: 0.0439 | Avg Loss: 0.0494\n",
      "  Batch   40 | Loss: 0.0439 | Avg Loss: 0.0494\n",
      "  Batch   60 | Loss: 0.0648 | Avg Loss: 0.0503\n",
      "  Batch   60 | Loss: 0.0648 | Avg Loss: 0.0503\n",
      "  Batch   80 | Loss: 0.0449 | Avg Loss: 0.0511\n",
      "  Batch   80 | Loss: 0.0449 | Avg Loss: 0.0511\n",
      "  Batch  100 | Loss: 0.0413 | Avg Loss: 0.0510\n",
      "  Batch  100 | Loss: 0.0413 | Avg Loss: 0.0510\n",
      "  Batch  120 | Loss: 0.0488 | Avg Loss: 0.0509\n",
      "  Batch  120 | Loss: 0.0488 | Avg Loss: 0.0509\n",
      "  Batch  140 | Loss: 0.0469 | Avg Loss: 0.0507\n",
      "  Batch  140 | Loss: 0.0469 | Avg Loss: 0.0507\n",
      "  Batch  160 | Loss: 0.0524 | Avg Loss: 0.0505\n",
      "  Batch  160 | Loss: 0.0524 | Avg Loss: 0.0505\n",
      "  Batch  180 | Loss: 0.0272 | Avg Loss: 0.0507\n",
      "  Batch  180 | Loss: 0.0272 | Avg Loss: 0.0507\n",
      "  Batch  200 | Loss: 0.0560 | Avg Loss: 0.0502\n",
      "  Batch  200 | Loss: 0.0560 | Avg Loss: 0.0502\n",
      "  Batch  220 | Loss: 0.0445 | Avg Loss: 0.0501\n",
      "  Batch  220 | Loss: 0.0445 | Avg Loss: 0.0501\n",
      "  Batch  240 | Loss: 0.0386 | Avg Loss: 0.0498\n",
      "  Batch  240 | Loss: 0.0386 | Avg Loss: 0.0498\n",
      "  Batch  260 | Loss: 0.0225 | Avg Loss: 0.0493\n",
      "  Batch  260 | Loss: 0.0225 | Avg Loss: 0.0493\n",
      "  Batch  280 | Loss: 0.0474 | Avg Loss: 0.0489\n",
      "  Batch  280 | Loss: 0.0474 | Avg Loss: 0.0489\n",
      "  Batch  300 | Loss: 0.0425 | Avg Loss: 0.0488\n",
      "  Batch  300 | Loss: 0.0425 | Avg Loss: 0.0488\n",
      "  Batch  320 | Loss: 0.0527 | Avg Loss: 0.0486\n",
      "  Batch  320 | Loss: 0.0527 | Avg Loss: 0.0486\n",
      "  Batch  340 | Loss: 0.0315 | Avg Loss: 0.0483\n",
      "  Batch  340 | Loss: 0.0315 | Avg Loss: 0.0483\n",
      "  Batch  360 | Loss: 0.0529 | Avg Loss: 0.0479\n",
      "  Batch  360 | Loss: 0.0529 | Avg Loss: 0.0479\n",
      "  Batch  380 | Loss: 0.0320 | Avg Loss: 0.0477\n",
      "  Batch  380 | Loss: 0.0320 | Avg Loss: 0.0477\n",
      "  Batch  400 | Loss: 0.0369 | Avg Loss: 0.0478\n",
      "  Batch  400 | Loss: 0.0369 | Avg Loss: 0.0478\n",
      "  Batch  420 | Loss: 0.0335 | Avg Loss: 0.0475\n",
      "  Batch  420 | Loss: 0.0335 | Avg Loss: 0.0475\n",
      "  Batch  440 | Loss: 0.0230 | Avg Loss: 0.0473\n",
      "  Batch  440 | Loss: 0.0230 | Avg Loss: 0.0473\n",
      "  Batch  460 | Loss: 0.0581 | Avg Loss: 0.0471\n",
      "  Batch  460 | Loss: 0.0581 | Avg Loss: 0.0471\n",
      "  Batch  480 | Loss: 0.0819 | Avg Loss: 0.0470\n",
      "  Batch  480 | Loss: 0.0819 | Avg Loss: 0.0470\n",
      "  Batch  500 | Loss: 0.0457 | Avg Loss: 0.0471\n",
      "  Batch  500 | Loss: 0.0457 | Avg Loss: 0.0471\n",
      "  Batch  520 | Loss: 0.0496 | Avg Loss: 0.0469\n",
      "  Batch  520 | Loss: 0.0496 | Avg Loss: 0.0469\n",
      "  Batch  540 | Loss: 0.0461 | Avg Loss: 0.0468\n",
      "  Batch  540 | Loss: 0.0461 | Avg Loss: 0.0468\n",
      "  Batch  560 | Loss: 0.0295 | Avg Loss: 0.0467\n",
      "  Batch  560 | Loss: 0.0295 | Avg Loss: 0.0467\n",
      "  Batch  580 | Loss: 0.0469 | Avg Loss: 0.0465\n",
      "  Batch  580 | Loss: 0.0469 | Avg Loss: 0.0465\n",
      "  Batch  600 | Loss: 0.0273 | Avg Loss: 0.0463\n",
      "  Batch  600 | Loss: 0.0273 | Avg Loss: 0.0463\n",
      "  Batch  620 | Loss: 0.0401 | Avg Loss: 0.0462\n",
      "  Batch  620 | Loss: 0.0401 | Avg Loss: 0.0462\n",
      "  Batch  640 | Loss: 0.0257 | Avg Loss: 0.0460\n",
      "  Batch  640 | Loss: 0.0257 | Avg Loss: 0.0460\n",
      "  Batch  660 | Loss: 0.0353 | Avg Loss: 0.0458\n",
      "  Batch  660 | Loss: 0.0353 | Avg Loss: 0.0458\n",
      "  Batch  680 | Loss: 0.0420 | Avg Loss: 0.0457\n",
      "  Batch  680 | Loss: 0.0420 | Avg Loss: 0.0457\n",
      "  Batch  700 | Loss: 0.0713 | Avg Loss: 0.0456\n",
      "  Batch  700 | Loss: 0.0713 | Avg Loss: 0.0456\n",
      "  Batch  720 | Loss: 0.0477 | Avg Loss: 0.0456\n",
      "  Batch  720 | Loss: 0.0477 | Avg Loss: 0.0456\n",
      "  Batch  740 | Loss: 0.0494 | Avg Loss: 0.0454\n",
      "  Batch  740 | Loss: 0.0494 | Avg Loss: 0.0454\n",
      "  Batch  760 | Loss: 0.0453 | Avg Loss: 0.0454\n",
      "  Batch  760 | Loss: 0.0453 | Avg Loss: 0.0454\n",
      "  Batch  780 | Loss: 0.0475 | Avg Loss: 0.0452\n",
      "  Batch  780 | Loss: 0.0475 | Avg Loss: 0.0452\n",
      "  Batch  800 | Loss: 0.0687 | Avg Loss: 0.0453\n",
      "  Batch  800 | Loss: 0.0687 | Avg Loss: 0.0453\n",
      "  Batch  820 | Loss: 0.0478 | Avg Loss: 0.0453\n",
      "  Batch  820 | Loss: 0.0478 | Avg Loss: 0.0453\n",
      "  Batch  840 | Loss: 0.0409 | Avg Loss: 0.0451\n",
      "  Batch  840 | Loss: 0.0409 | Avg Loss: 0.0451\n",
      "  Batch  860 | Loss: 0.0341 | Avg Loss: 0.0450\n",
      "  Batch  860 | Loss: 0.0341 | Avg Loss: 0.0450\n",
      "  Batch  880 | Loss: 0.0450 | Avg Loss: 0.0448\n",
      "  Batch  880 | Loss: 0.0450 | Avg Loss: 0.0448\n",
      "  Batch  900 | Loss: 0.0487 | Avg Loss: 0.0448\n",
      "  Batch  900 | Loss: 0.0487 | Avg Loss: 0.0448\n",
      "  Batch  920 | Loss: 0.0228 | Avg Loss: 0.0445\n",
      "  Batch  920 | Loss: 0.0228 | Avg Loss: 0.0445\n",
      "  Batch  940 | Loss: 0.0465 | Avg Loss: 0.0445\n",
      "  Batch  940 | Loss: 0.0465 | Avg Loss: 0.0445\n",
      "  Batch  960 | Loss: 0.0645 | Avg Loss: 0.0444\n",
      "  Batch  960 | Loss: 0.0645 | Avg Loss: 0.0444\n",
      "  Batch  980 | Loss: 0.0269 | Avg Loss: 0.0443\n",
      "  Batch  980 | Loss: 0.0269 | Avg Loss: 0.0443\n",
      "  Batch 1000 | Loss: 0.0295 | Avg Loss: 0.0441\n",
      "  Batch 1000 | Loss: 0.0295 | Avg Loss: 0.0441\n",
      "  Batch 1020 | Loss: 0.0497 | Avg Loss: 0.0440\n",
      "  Batch 1020 | Loss: 0.0497 | Avg Loss: 0.0440\n",
      "  Batch 1040 | Loss: 0.0328 | Avg Loss: 0.0439\n",
      "  Batch 1040 | Loss: 0.0328 | Avg Loss: 0.0439\n",
      "  Batch 1060 | Loss: 0.0354 | Avg Loss: 0.0438\n",
      "  Batch 1060 | Loss: 0.0354 | Avg Loss: 0.0438\n",
      "  Batch 1080 | Loss: 0.0561 | Avg Loss: 0.0437\n",
      "  Batch 1080 | Loss: 0.0561 | Avg Loss: 0.0437\n",
      "  Batch 1100 | Loss: 0.0628 | Avg Loss: 0.0436\n",
      "  Batch 1100 | Loss: 0.0628 | Avg Loss: 0.0436\n",
      "  Batch 1120 | Loss: 0.0600 | Avg Loss: 0.0435\n",
      "  Batch 1120 | Loss: 0.0600 | Avg Loss: 0.0435\n",
      "  Batch 1140 | Loss: 0.0307 | Avg Loss: 0.0434\n",
      "  Batch 1140 | Loss: 0.0307 | Avg Loss: 0.0434\n",
      "  Batch 1160 | Loss: 0.0451 | Avg Loss: 0.0434\n",
      "  Batch 1160 | Loss: 0.0451 | Avg Loss: 0.0434\n",
      "  Batch 1180 | Loss: 0.0350 | Avg Loss: 0.0433\n",
      "  Batch 1180 | Loss: 0.0350 | Avg Loss: 0.0433\n",
      "  Batch 1200 | Loss: 0.0273 | Avg Loss: 0.0432\n",
      "  Batch 1200 | Loss: 0.0273 | Avg Loss: 0.0432\n",
      "  Batch 1220 | Loss: 0.0492 | Avg Loss: 0.0431\n",
      "  Batch 1220 | Loss: 0.0492 | Avg Loss: 0.0431\n",
      "  Batch 1240 | Loss: 0.0514 | Avg Loss: 0.0430\n",
      "  Batch 1240 | Loss: 0.0514 | Avg Loss: 0.0430\n",
      "Epoch 1 Complete | Avg Loss: 0.0430 | Time: 661.4s\n",
      "\n",
      "--- Epoch 2/3 ---\n",
      "Epoch 1 Complete | Avg Loss: 0.0430 | Time: 661.4s\n",
      "\n",
      "--- Epoch 2/3 ---\n",
      "  Batch   20 | Loss: 0.0207 | Avg Loss: 0.0373\n",
      "  Batch   20 | Loss: 0.0207 | Avg Loss: 0.0373\n",
      "  Batch   40 | Loss: 0.0423 | Avg Loss: 0.0366\n",
      "  Batch   40 | Loss: 0.0423 | Avg Loss: 0.0366\n",
      "  Batch   60 | Loss: 0.0233 | Avg Loss: 0.0350\n",
      "  Batch   60 | Loss: 0.0233 | Avg Loss: 0.0350\n",
      "  Batch   80 | Loss: 0.0410 | Avg Loss: 0.0338\n",
      "  Batch   80 | Loss: 0.0410 | Avg Loss: 0.0338\n",
      "  Batch  100 | Loss: 0.0471 | Avg Loss: 0.0338\n",
      "  Batch  100 | Loss: 0.0471 | Avg Loss: 0.0338\n",
      "  Batch  120 | Loss: 0.0358 | Avg Loss: 0.0340\n",
      "  Batch  120 | Loss: 0.0358 | Avg Loss: 0.0340\n",
      "  Batch  140 | Loss: 0.0250 | Avg Loss: 0.0336\n",
      "  Batch  140 | Loss: 0.0250 | Avg Loss: 0.0336\n",
      "  Batch  160 | Loss: 0.0519 | Avg Loss: 0.0338\n",
      "  Batch  160 | Loss: 0.0519 | Avg Loss: 0.0338\n",
      "  Batch  180 | Loss: 0.0547 | Avg Loss: 0.0341\n",
      "  Batch  180 | Loss: 0.0547 | Avg Loss: 0.0341\n",
      "  Batch  200 | Loss: 0.0379 | Avg Loss: 0.0339\n",
      "  Batch  200 | Loss: 0.0379 | Avg Loss: 0.0339\n",
      "  Batch  220 | Loss: 0.0373 | Avg Loss: 0.0341\n",
      "  Batch  220 | Loss: 0.0373 | Avg Loss: 0.0341\n",
      "  Batch  240 | Loss: 0.0537 | Avg Loss: 0.0342\n",
      "  Batch  240 | Loss: 0.0537 | Avg Loss: 0.0342\n",
      "  Batch  260 | Loss: 0.0369 | Avg Loss: 0.0343\n",
      "  Batch  260 | Loss: 0.0369 | Avg Loss: 0.0343\n",
      "  Batch  280 | Loss: 0.0391 | Avg Loss: 0.0344\n",
      "  Batch  280 | Loss: 0.0391 | Avg Loss: 0.0344\n",
      "  Batch  300 | Loss: 0.0295 | Avg Loss: 0.0345\n",
      "  Batch  300 | Loss: 0.0295 | Avg Loss: 0.0345\n",
      "  Batch  320 | Loss: 0.0381 | Avg Loss: 0.0344\n",
      "  Batch  320 | Loss: 0.0381 | Avg Loss: 0.0344\n",
      "  Batch  340 | Loss: 0.0269 | Avg Loss: 0.0347\n",
      "  Batch  360 | Loss: 0.0482 | Avg Loss: 0.0348\n",
      "  Batch  380 | Loss: 0.0278 | Avg Loss: 0.0347\n",
      "  Batch  400 | Loss: 0.0627 | Avg Loss: 0.0347\n",
      "  Batch  420 | Loss: 0.0298 | Avg Loss: 0.0348\n",
      "  Batch  440 | Loss: 0.0382 | Avg Loss: 0.0347\n",
      "  Batch  460 | Loss: 0.0276 | Avg Loss: 0.0346\n",
      "  Batch  480 | Loss: 0.0513 | Avg Loss: 0.0345\n",
      "  Batch  500 | Loss: 0.0459 | Avg Loss: 0.0346\n",
      "  Batch  520 | Loss: 0.0392 | Avg Loss: 0.0346\n",
      "  Batch  540 | Loss: 0.0322 | Avg Loss: 0.0344\n",
      "  Batch  560 | Loss: 0.0360 | Avg Loss: 0.0344\n",
      "  Batch  580 | Loss: 0.0255 | Avg Loss: 0.0344\n",
      "  Batch  600 | Loss: 0.0471 | Avg Loss: 0.0345\n",
      "  Batch  620 | Loss: 0.0332 | Avg Loss: 0.0345\n",
      "  Batch  640 | Loss: 0.0356 | Avg Loss: 0.0346\n",
      "  Batch  660 | Loss: 0.0357 | Avg Loss: 0.0347\n",
      "  Batch  680 | Loss: 0.0286 | Avg Loss: 0.0347\n",
      "  Batch  700 | Loss: 0.0224 | Avg Loss: 0.0348\n",
      "  Batch  720 | Loss: 0.0404 | Avg Loss: 0.0348\n",
      "  Batch  740 | Loss: 0.0318 | Avg Loss: 0.0349\n",
      "  Batch  760 | Loss: 0.0188 | Avg Loss: 0.0349\n",
      "  Batch  780 | Loss: 0.0323 | Avg Loss: 0.0349\n",
      "  Batch  800 | Loss: 0.0365 | Avg Loss: 0.0349\n",
      "  Batch  820 | Loss: 0.0287 | Avg Loss: 0.0348\n",
      "  Batch  840 | Loss: 0.0475 | Avg Loss: 0.0348\n",
      "  Batch  860 | Loss: 0.0328 | Avg Loss: 0.0348\n",
      "  Batch  880 | Loss: 0.0268 | Avg Loss: 0.0349\n",
      "  Batch  900 | Loss: 0.0540 | Avg Loss: 0.0349\n",
      "  Batch  920 | Loss: 0.0235 | Avg Loss: 0.0350\n",
      "  Batch  940 | Loss: 0.0298 | Avg Loss: 0.0349\n",
      "  Batch  960 | Loss: 0.0390 | Avg Loss: 0.0348\n",
      "  Batch  980 | Loss: 0.0322 | Avg Loss: 0.0348\n",
      "  Batch 1000 | Loss: 0.0338 | Avg Loss: 0.0347\n",
      "  Batch 1020 | Loss: 0.0238 | Avg Loss: 0.0347\n",
      "  Batch 1040 | Loss: 0.0227 | Avg Loss: 0.0347\n",
      "  Batch 1060 | Loss: 0.0354 | Avg Loss: 0.0347\n",
      "  Batch 1080 | Loss: 0.0419 | Avg Loss: 0.0347\n",
      "  Batch 1100 | Loss: 0.0135 | Avg Loss: 0.0347\n",
      "  Batch 1120 | Loss: 0.0556 | Avg Loss: 0.0347\n",
      "  Batch 1140 | Loss: 0.0193 | Avg Loss: 0.0346\n",
      "  Batch 1160 | Loss: 0.0350 | Avg Loss: 0.0347\n",
      "  Batch 1180 | Loss: 0.0520 | Avg Loss: 0.0346\n",
      "  Batch 1200 | Loss: 0.0280 | Avg Loss: 0.0347\n",
      "  Batch 1220 | Loss: 0.0405 | Avg Loss: 0.0347\n",
      "  Batch 1240 | Loss: 0.0286 | Avg Loss: 0.0347\n",
      "Epoch 2 Complete | Avg Loss: 0.0347 | Time: 624.8s\n",
      "\n",
      "--- Epoch 3/3 ---\n",
      "  Batch   20 | Loss: 0.0291 | Avg Loss: 0.0264\n",
      "  Batch   40 | Loss: 0.0289 | Avg Loss: 0.0285\n",
      "  Batch   60 | Loss: 0.0289 | Avg Loss: 0.0283\n",
      "  Batch   80 | Loss: 0.0272 | Avg Loss: 0.0282\n",
      "  Batch  100 | Loss: 0.0187 | Avg Loss: 0.0280\n",
      "  Batch  120 | Loss: 0.0430 | Avg Loss: 0.0285\n",
      "  Batch  140 | Loss: 0.0290 | Avg Loss: 0.0282\n",
      "  Batch  160 | Loss: 0.0261 | Avg Loss: 0.0282\n",
      "  Batch  180 | Loss: 0.0421 | Avg Loss: 0.0285\n",
      "  Batch  200 | Loss: 0.0300 | Avg Loss: 0.0285\n",
      "  Batch  220 | Loss: 0.0200 | Avg Loss: 0.0286\n",
      "  Batch  240 | Loss: 0.0301 | Avg Loss: 0.0286\n",
      "  Batch  260 | Loss: 0.0330 | Avg Loss: 0.0285\n",
      "  Batch  280 | Loss: 0.0211 | Avg Loss: 0.0285\n",
      "  Batch  300 | Loss: 0.0363 | Avg Loss: 0.0288\n",
      "  Batch  320 | Loss: 0.0243 | Avg Loss: 0.0289\n",
      "  Batch  340 | Loss: 0.0530 | Avg Loss: 0.0291\n",
      "  Batch  360 | Loss: 0.0208 | Avg Loss: 0.0291\n",
      "  Batch  380 | Loss: 0.0141 | Avg Loss: 0.0291\n",
      "  Batch  400 | Loss: 0.0312 | Avg Loss: 0.0290\n",
      "  Batch  420 | Loss: 0.0286 | Avg Loss: 0.0290\n",
      "  Batch  440 | Loss: 0.0302 | Avg Loss: 0.0290\n",
      "  Batch  460 | Loss: 0.0147 | Avg Loss: 0.0290\n",
      "  Batch  480 | Loss: 0.0499 | Avg Loss: 0.0291\n",
      "  Batch  500 | Loss: 0.0272 | Avg Loss: 0.0292\n",
      "  Batch  520 | Loss: 0.0302 | Avg Loss: 0.0291\n",
      "  Batch  540 | Loss: 0.0443 | Avg Loss: 0.0292\n",
      "  Batch  560 | Loss: 0.0298 | Avg Loss: 0.0292\n",
      "  Batch  580 | Loss: 0.0143 | Avg Loss: 0.0292\n",
      "  Batch  600 | Loss: 0.0198 | Avg Loss: 0.0291\n",
      "  Batch  620 | Loss: 0.0450 | Avg Loss: 0.0293\n",
      "  Batch  640 | Loss: 0.0360 | Avg Loss: 0.0292\n",
      "  Batch  660 | Loss: 0.0276 | Avg Loss: 0.0292\n",
      "  Batch  680 | Loss: 0.0349 | Avg Loss: 0.0293\n",
      "  Batch  700 | Loss: 0.0359 | Avg Loss: 0.0293\n",
      "  Batch  720 | Loss: 0.0290 | Avg Loss: 0.0294\n",
      "  Batch  740 | Loss: 0.0260 | Avg Loss: 0.0294\n",
      "  Batch  760 | Loss: 0.0183 | Avg Loss: 0.0295\n",
      "  Batch  780 | Loss: 0.0211 | Avg Loss: 0.0295\n",
      "  Batch  800 | Loss: 0.0324 | Avg Loss: 0.0295\n",
      "  Batch  820 | Loss: 0.0144 | Avg Loss: 0.0295\n",
      "  Batch  840 | Loss: 0.0295 | Avg Loss: 0.0296\n",
      "  Batch  860 | Loss: 0.0527 | Avg Loss: 0.0296\n",
      "  Batch  880 | Loss: 0.0228 | Avg Loss: 0.0296\n",
      "  Batch  900 | Loss: 0.0200 | Avg Loss: 0.0296\n",
      "  Batch  920 | Loss: 0.0387 | Avg Loss: 0.0296\n",
      "  Batch  940 | Loss: 0.0425 | Avg Loss: 0.0295\n",
      "  Batch  960 | Loss: 0.0229 | Avg Loss: 0.0295\n",
      "  Batch  980 | Loss: 0.0299 | Avg Loss: 0.0295\n",
      "  Batch 1000 | Loss: 0.0369 | Avg Loss: 0.0295\n",
      "  Batch 1020 | Loss: 0.0441 | Avg Loss: 0.0295\n",
      "  Batch 1040 | Loss: 0.0260 | Avg Loss: 0.0295\n",
      "  Batch 1060 | Loss: 0.0363 | Avg Loss: 0.0295\n",
      "  Batch 1080 | Loss: 0.0319 | Avg Loss: 0.0296\n",
      "  Batch 1100 | Loss: 0.0248 | Avg Loss: 0.0297\n",
      "  Batch 1120 | Loss: 0.0284 | Avg Loss: 0.0297\n",
      "  Batch 1140 | Loss: 0.0410 | Avg Loss: 0.0297\n",
      "  Batch 1160 | Loss: 0.0241 | Avg Loss: 0.0297\n",
      "  Batch 1180 | Loss: 0.0388 | Avg Loss: 0.0297\n",
      "  Batch 1200 | Loss: 0.0200 | Avg Loss: 0.0297\n",
      "  Batch 1220 | Loss: 0.0218 | Avg Loss: 0.0297\n",
      "  Batch 1240 | Loss: 0.0328 | Avg Loss: 0.0298\n",
      "Epoch 3 Complete | Avg Loss: 0.0298 | Time: 632.8s\n",
      "Total training time: 1919.0 seconds (31.98 minutes)\n",
      "Average time per epoch: 639.7 seconds\n"
     ]
    }
   ],
   "source": [
    "# Step 14: Training loop with mixed precision (AMP) and timing\n",
    "import time\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Clear any leftover GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "scaler = GradScaler() if torch.cuda.is_available() else None\n",
    "\n",
    "print_once(\"training_start_sep\", \"=\"*70)\n",
    "print_once(\"training_start_title\", \"STARTING TRAINING\")\n",
    "print_once(\"training_start_sep\", \"=\"*70)\n",
    "\n",
    "model.train()\n",
    "training_start = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---')\n",
    "    epoch_start = time.time()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "        attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "        labels = batch['labels'].to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            with autocast():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                loss = loss_fn(logits, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Print frequently to observe progress; reduce frequency if noisy\n",
    "        if (batch_idx + 1) % 20 == 0:\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            print(f'  Batch {batch_idx+1:4d} | Loss: {loss.item():.4f} | Avg Loss: {avg_loss:.4f}')\n",
    "\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    epoch_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1} Complete | Avg Loss: {epoch_loss:.4f} | Time: {epoch_time:.1f}s')\n",
    "\n",
    "total_time = time.time() - training_start\n",
    "print_once(\"training_end_sep\", '\\n' + '='*70)\n",
    "print_once(\"training_end_title\", 'TRAINING COMPLETE')\n",
    "print_once(\"training_end_sep\", '='*70)\n",
    "print(f'Total training time: {total_time:.1f} seconds ({total_time/60:.2f} minutes)')\n",
    "print(f'Average time per epoch: {total_time/NUM_EPOCHS:.1f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c994c0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Output directories created\n"
     ]
    }
   ],
   "source": [
    "# Step 15: Create output directories\n",
    "model_dir = Path(\"toxic_model\")\n",
    "tokenizer_dir = Path(\"toxic_tokenizer\")\n",
    "\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "tokenizer_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"✓ Output directories created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55c0d8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model saved to: toxic_model\n",
      "  Files: [WindowsPath('toxic_model/config.json'), WindowsPath('toxic_model/model.safetensors')]\n"
     ]
    }
   ],
   "source": [
    "# Step 16: Save model\n",
    "model.save_pretrained(model_dir)\n",
    "\n",
    "print(f\"✓ Model saved to: {model_dir}\")\n",
    "print(f\"  Files: {list(model_dir.glob('*'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "062db77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenizer saved to: toxic_tokenizer\n",
      "  Files: [WindowsPath('toxic_tokenizer/special_tokens_map.json'), WindowsPath('toxic_tokenizer/tokenizer.json'), WindowsPath('toxic_tokenizer/tokenizer_config.json'), WindowsPath('toxic_tokenizer/vocab.txt')]\n"
     ]
    }
   ],
   "source": [
    "# Step 17: Save tokenizer\n",
    "tokenizer.save_pretrained(tokenizer_dir)\n",
    "\n",
    "print(f\"✓ Tokenizer saved to: {tokenizer_dir}\")\n",
    "print(f\"  Files: {list(tokenizer_dir.glob('*'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "12f7503c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Inference function created\n"
     ]
    }
   ],
   "source": [
    "# Step 18: Create inference function\n",
    "def predict(text, model_path=\"toxic_model\", tokenizer_path=\"toxic_tokenizer\"):\n",
    "    \"\"\"\n",
    "    Predict toxicity probabilities for input text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input comment text\n",
    "        model_path (str): Path to saved model\n",
    "        tokenizer_path (str): Path to saved tokenizer\n",
    "    \n",
    "    Returns:\n",
    "        dict: Text and probabilities for each label\n",
    "    \"\"\"\n",
    "    # Load model and tokenizer\n",
    "    loaded_model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "    loaded_tokenizer = DistilBertTokenizerFast.from_pretrained(tokenizer_path)\n",
    "    \n",
    "    loaded_model.eval()\n",
    "    loaded_model.to(device)\n",
    "    \n",
    "    # Preprocess and tokenize\n",
    "    text = preprocess_text(text)\n",
    "    inputs = loaded_tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "    \n",
    "    # Return probabilities for each label\n",
    "    label_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "    return {\n",
    "        \"text\": text[:100] + \"...\" if len(text) > 100 else text,\n",
    "        \"probabilities\": {label: float(prob) for label, prob in zip(label_cols, probs)},\n",
    "    }\n",
    "\n",
    "print(\"✓ Inference function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2d73eddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTING INFERENCE\n",
      "======================================================================\n",
      "\n",
      "--- Test 1 ---\n",
      "Input: This is a great movie! I loved it.\n",
      "Probabilities:\n",
      "  toxic          : 0.0016 \n",
      "  severe_toxic   : 0.0001 \n",
      "  obscene        : 0.0004 \n",
      "  threat         : 0.0001 \n",
      "  insult         : 0.0002 \n",
      "  identity_hate  : 0.0001 \n",
      "\n",
      "--- Test 2 ---\n",
      "Input: You are an idiot and should die.\n",
      "Probabilities:\n",
      "  toxic          : 0.9911 ███████████████████\n",
      "  severe_toxic   : 0.4456 ████████\n",
      "  obscene        : 0.7602 ███████████████\n",
      "  threat         : 0.8043 ████████████████\n",
      "  insult         : 0.9155 ██████████████████\n",
      "  identity_hate  : 0.0931 █\n",
      "\n",
      "--- Test 3 ---\n",
      "Input: I hate people like you, they are disgusting.\n",
      "Probabilities:\n",
      "  toxic          : 0.9819 ███████████████████\n",
      "  severe_toxic   : 0.0148 \n",
      "  obscene        : 0.0285 \n",
      "  threat         : 0.0383 \n",
      "  insult         : 0.3978 ███████\n",
      "  identity_hate  : 0.1695 ███\n",
      "\n",
      "--- Test 4 ---\n",
      "Input: The weather is nice today.\n",
      "Probabilities:\n",
      "  toxic          : 0.0004 \n",
      "  severe_toxic   : 0.0000 \n",
      "  obscene        : 0.0002 \n",
      "  threat         : 0.0001 \n",
      "  insult         : 0.0001 \n",
      "  identity_hate  : 0.0001 \n",
      "\n",
      "--- Test 5 ---\n",
      "Input: I hope you get hit by a car.\n",
      "Probabilities:\n",
      "  toxic          : 0.9153 ██████████████████\n",
      "  severe_toxic   : 0.1763 ███\n",
      "  obscene        : 0.1414 ██\n",
      "  threat         : 0.8525 █████████████████\n",
      "  insult         : 0.1438 ██\n",
      "  identity_hate  : 0.0464 \n"
     ]
    }
   ],
   "source": [
    "# Step 19: Test inference on sample texts\n",
    "print(\"=\"*70)\n",
    "print(\"TESTING INFERENCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_texts = [\n",
    "    \"This is a great movie! I loved it.\",\n",
    "    \"You are an idiot and should die.\",\n",
    "    \"I hate people like you, they are disgusting.\",\n",
    "    \"The weather is nice today.\",\n",
    "    \"I hope you get hit by a car.\",\n",
    "]\n",
    "\n",
    "for i, test_text in enumerate(test_texts, 1):\n",
    "    print(f\"\\n--- Test {i} ---\")\n",
    "    result = predict(test_text)\n",
    "    print(f\"Input: {result['text']}\")\n",
    "    print(\"Probabilities:\")\n",
    "    for label, prob in result['probabilities'].items():\n",
    "        bar = \"█\" * int(prob * 20)\n",
    "        print(f\"  {label:15s}: {prob:.4f} {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d1ee0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
